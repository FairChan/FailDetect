{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enviroment Used:\n",
    "\n",
    "1. Python 3.11.5\n",
    "2. Required libraries: \n",
    "    pandas===1.5.3, \n",
    "    numpy===1.24.3, \n",
    "    scikit-learn===1.2.2, \n",
    "    torch===2.1.1, \n",
    "    torch-tabnet===4.0.0, \n",
    "    matplotlib===3.7.2, \n",
    "    seaborn===0.12.2\n",
    "\n",
    "We recommend ensuring your environment matches our experimental setup (same Python version and library versions) to facilitate running the code.\n",
    "Therefore, you can directly install via pip using the Jupyter notebook below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code Summary:\n",
    "Installs specific versions of required libraries (pandas, numpy, torch, tabnet, catboost) to ensure environment reproducibility.\n",
    "\"\"\"\n",
    "!pip install pandas==1.5.3 numpy==1.24.3 scikit-learn==1.2.2 torch==2.1.1 pytorch-tabnet==4.0.0 matplotlib==3.7.2 seaborn==0.12.2 catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1. Data Processing \n",
    "import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code Summary:\n",
    "Imports necessary Python libraries for data manipulation, machine learning (CatBoost, TabNet), and visualization.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split,RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines file paths for the raw input data, the codebook, and the processed output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = r'dataProcessing/Original.csv'\n",
    "codebook_path = r'codebook/FinalCodeBook.csv'\n",
    "output_csv = r'dataProcessing/Processed_Original.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Codebook and Parse Metadata\n",
    "Loads the codebook, cleans column names, and identifies target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Loading codebook...\")\n",
    "codebook = pd.read_csv(codebook_path, encoding='latin-1')\n",
    "# Clean columns (BOM, whitespace)\n",
    "codebook.columns = codebook.columns.str.strip().str.replace('^ï»¿', '', regex=True)\n",
    "\n",
    "# Extract target variables from the codebook\n",
    "if 'Variable' in codebook.columns:\n",
    "    target_vars = codebook['Variable'].tolist()\n",
    "else:\n",
    "    print(\"Warning: 'Variable' column not found by name. Using first column.\")\n",
    "    target_vars = codebook.iloc[:, 0].tolist()\n",
    "print(f\"Found {len(target_vars)} target variables in codebook.\")\n",
    "\n",
    "# Parse missing values\n",
    "# Example string: \"9: Omitted or invalid; Sysmis: Not administered\"\n",
    "# We want to map {col_name: [9, 'Sysmis']}\n",
    "missing_map = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines a helper function to parse the missing value schemes (e.g., '9: Omitted') from the codebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_missing_scheme(scheme_str):\n",
    "    missing_vals = []\n",
    "    if pd.isna(scheme_str):\n",
    "        return missing_vals\n",
    "    \n",
    "    # Split by semicolon\n",
    "    parts = str(scheme_str).split(';')\n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        # Look for \"Value: Label\" pattern\n",
    "        if ':' in part:\n",
    "            val_str = part.split(':')[0].strip()\n",
    "            # Try to convert to float/int if possible\n",
    "            try:\n",
    "                val = float(val_str)\n",
    "                # Check if it's an integer\n",
    "                if val.is_integer():\n",
    "                    missing_vals.append(int(val))\n",
    "                else:\n",
    "                    missing_vals.append(val)\n",
    "            except ValueError:\n",
    "                # Keep as string if not numeric (e.g. 'Sysmis' though usually handled separately)\n",
    "                if val_str.lower() != 'sysmis': # Sysmis is usually auto-handled or empty in CSV\n",
    "                     missing_vals.append(val_str)\n",
    "    return missing_vals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterates through the codebook to build a dictionary mapping variables to their specific missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in codebook.iterrows():\n",
    "    var = row['Variable']\n",
    "    scheme = row.get('Missing Scheme Detailed: SPSS', '')\n",
    "    vals = parse_missing_scheme(scheme)\n",
    "    if vals:\n",
    "        missing_map[var] = vals\n",
    "\n",
    "print(\"Missing value schemes parsed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets up parameters for chunked processing and detects the CSV header encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 50000  # Adjust based on memory\n",
    "first_chunk = True\n",
    "\n",
    "print(f\"Processing {input_csv} in chunks of {chunksize}...\")\n",
    "\n",
    "# Determine columns to keep: IDs + Target Vars\n",
    "# Try reading with utf-8-sig to handle BOM\n",
    "try:\n",
    "    header = pd.read_csv(input_csv, nrows=0, encoding='utf-8-sig').columns.tolist()\n",
    "except UnicodeDecodeError:\n",
    "    print(\"utf-8-sig failed, trying latin-1 for header...\")\n",
    "    header = pd.read_csv(input_csv, nrows=0, encoding='latin-1').columns.tolist()\n",
    "\n",
    "print(f\"Detected header columns: {header[:5]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifies columns to keep: IDs, weights, and target variables, removing duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_vars = [col for col in header if col.startswith('ID')]\n",
    "# Also keep weights if present, usually important (MATWGT, etc.)\n",
    "weight_vars = [col for col in header if 'WGT' in col or 'JK' in col]\n",
    "\n",
    "# Combine keep list\n",
    "keep_cols = id_vars + weight_vars + target_vars\n",
    "# Ensure we only keep columns that actually exist in the CSV\n",
    "keep_cols = [c for c in keep_cols if c in header]\n",
    "# Remove duplicates\n",
    "keep_cols = list(dict.fromkeys(keep_cols))\n",
    "\n",
    "print(f\"Keeping {len(keep_cols)} columns: {keep_cols[:10]} ...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processes the large CSV in chunks: filters columns, handles missing values based on the codebook, and saves to a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_count = 0\n",
    "\n",
    "try:\n",
    "    # Use the same encoding that worked for header\n",
    "    encoding = 'utf-8-sig'\n",
    "    try:\n",
    "        pd.read_csv(input_csv, nrows=1, encoding=encoding)\n",
    "    except:\n",
    "        encoding = 'latin-1'\n",
    "\n",
    "    with pd.read_csv(input_csv, chunksize=chunksize, usecols=keep_cols, encoding=encoding, low_memory=False) as reader:\n",
    "        for chunk in reader:\n",
    "            # Apply missing value handling\n",
    "            for col in chunk.columns:\n",
    "                if col in missing_map:\n",
    "                    # Replace defined missing values with NaN\n",
    "                    # chunk[col].replace(missing_map[col], np.nan, inplace=True) # deprecated\n",
    "                    mask = chunk[col].isin(missing_map[col])\n",
    "                    if mask.any():\n",
    "                         chunk.loc[mask, col] = np.nan\n",
    "            \n",
    "            # Save to file\n",
    "            mode = 'w' if first_chunk else 'a'\n",
    "            header_arg = first_chunk\n",
    "            chunk.to_csv(output_csv, index=False, mode=mode, header=header_arg)\n",
    "            \n",
    "            processed_count += len(chunk)\n",
    "            first_chunk = False\n",
    "            print(f\"Processed {processed_count} rows...\")\n",
    "\n",
    "    print(f\"Done! Processed data saved to {output_csv}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Merging & Target Definition (2merge_fail_data.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines file paths for the Data Merging phase (Processed data + Original Results).Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_csv_path = r'dataProcessing/Processed_Original.csv'\n",
    "result_csv_path = r'dataProcessing/OriginalResult.csv'\n",
    "output_csv_path = r'dataProcessing/Final.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the processed survey data and the student result data (Math scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading datasets...\")\n",
    "# Load processed data\n",
    "df_processed = pd.read_csv(processed_csv_path, low_memory=False)\n",
    "print(f\"Processed data loaded: {len(df_processed)} rows.\")\n",
    "\n",
    "# Load result data\n",
    "# We only need IDSTUD and the Math PVs to determine fail status\n",
    "cols_to_use = ['IDCNTRY', 'IDSTUD', 'BSMMAT01', 'BSMMAT02', 'BSMMAT03', 'BSMMAT04', 'BSMMAT05']\n",
    "df_result = pd.read_csv(result_csv_path, usecols=cols_to_use, low_memory=False)\n",
    "print(f\"Result data loaded: {len(df_result)} rows.\")\n",
    "\n",
    "# Deduplicate result data\n",
    "# OriginalResult.csv contains multiple rows per student (e.g. linked to different teachers)\n",
    "# We drop duplicates based on IDSTUD, keeping the first occurrence (scores are invariant for the student)\n",
    "df_result_unique = df_result.drop_duplicates(subset=['IDCNTRY', 'IDSTUD'])\n",
    "print(f\"Unique students in result data: {len(df_result_unique)}\")\n",
    "\n",
    "# Define Fail Standard\n",
    "# Standard: Average Math Plausible Value < 400 (Below Low International Benchmark)\n",
    "print(\"Calculating Fail status...\")\n",
    "pv_cols = ['BSMMAT01', 'BSMMAT02', 'BSMMAT03', 'BSMMAT04', 'BSMMAT05']\n",
    "df_result_unique['Math_Mean'] = df_result_unique[pv_cols].mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the 'Fail' status: Average Math Plausible Value < 400 indicates failure (1), otherwise pass (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Fail column\n",
    "# 1 = Fail (Mean Score < 400)\n",
    "# 0 = Pass (Mean Score >= 400)\n",
    "df_result_unique['Fail'] = np.where(df_result_unique['Math_Mean'] < 400, 1, 0)\n",
    "\n",
    "print(f\"Fail counts:\\n{df_result_unique['Fail'].value_counts()}\")\n",
    "\n",
    "# Merge\n",
    "print(\"Merging datasets...\")\n",
    "# Left merge to keep all students in the processed file\n",
    "# Using IDCNTRY and IDSTUD as keys\n",
    "df_merged = pd.merge(df_processed, \n",
    "                     df_result_unique[['IDCNTRY', 'IDSTUD', 'Math_Mean', 'Fail']], \n",
    "                     on=['IDCNTRY', 'IDSTUD'], \n",
    "                     how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks for students without scores, warns if any, and saves the final merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for unmerged students\n",
    "missing_scores = df_merged['Fail'].isna().sum()\n",
    "if missing_scores > 0:\n",
    "    print(f\"Warning: {missing_scores} students in Processed_Original.csv did not have scores in OriginalResult.csv\")\n",
    "\n",
    "# Save\n",
    "try:\n",
    "    df_merged.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Merged data saved to {output_csv_path}\")\n",
    "except PermissionError:\n",
    "    print(f\"Error: Could not save to {output_csv_path}. File might be open.\")\n",
    "    temp_output = output_csv_path.replace('.csv', '_new.csv')\n",
    "    df_merged.to_csv(temp_output, index=False)\n",
    "    print(f\"Saved to {temp_output} instead.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloads the codebook to prepare for updating it with the new 'Fail' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Codebook with Fail definition\n",
    "codebook_path = r'codebook/FinalCodeBook.csv'\n",
    "print(f\"Updating codebook at {codebook_path}...\")\n",
    "\n",
    "try:\n",
    "    codebook_df = pd.read_csv(codebook_path, encoding='utf-8-sig')\n",
    "except:\n",
    "    codebook_df = pd.read_csv(codebook_path, encoding='latin-1')\n",
    "\n",
    "# Clean column names to ensure matching\n",
    "codebook_df.columns = codebook_df.columns.str.strip().str.replace('^ï»¿', '', regex=True)\n",
    "var_col = codebook_df.columns[0] # Assuming first column is Variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adds the 'Fail' variable definition and metadata to the codebook if it is not already present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Fail variable already exists\n",
    "if 'Fail' not in codebook_df[var_col].astype(str).values:\n",
    "    # Construct new row dictionary matching the columns\n",
    "    new_row = {col: '' for col in codebook_df.columns}\n",
    "    \n",
    "    new_row[codebook_df.columns[0]] = 'Fail' # Variable\n",
    "    new_row[codebook_df.columns[1]] = 'Student Failure Status (Math Mean < 400)' # Label\n",
    "    new_row[codebook_df.columns[2]] = 'Derived' # Question Location\n",
    "    new_row[codebook_df.columns[3]] = 'Nominal' # Level\n",
    "    new_row[codebook_df.columns[4]] = '1' # Width\n",
    "    new_row[codebook_df.columns[5]] = '0' # Decimals\n",
    "    new_row[codebook_df.columns[6]] = '0' # Range Minimum\n",
    "    new_row[codebook_df.columns[7]] = '1' # Range Maximum\n",
    "    new_row[codebook_df.columns[8]] = '0: Pass; 1: Fail' # Value Scheme Detailed\n",
    "    new_row[codebook_df.columns[13]] = 'Derived' # Domain\n",
    "    new_row[codebook_df.columns[14]] = 'D' # Variable Class\n",
    "    new_row[codebook_df.columns[15]] = 'Derived from BSMMAT01-05' # Comment\n",
    "    \n",
    "    # Create DataFrame for new row\n",
    "    new_row_df = pd.DataFrame([new_row])\n",
    "    \n",
    "    # Append\n",
    "    codebook_df = pd.concat([codebook_df, new_row_df], ignore_index=True)\n",
    "    \n",
    "    # Save\n",
    "    codebook_df.to_csv(codebook_path, index=False, encoding='utf-8-sig')\n",
    "    print(\"Added 'Fail' variable to Codebook.\")\n",
    "else:\n",
    "    print(\"'Fail' variable already exists in Codebook.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CatBoost Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Summary: Hyperparameter Tuning\n",
    "According to the **Hyperparameter Tuning Results** table (Section 6.2), this module executes a randomized search to optimize the model.\n",
    "- **Goal:** Find the best combination of `n_d`, `n_steps`, `gamma`, etc.\n",
    "- **Key Insight:** Trial 5 yielded the best performance with **Val AUC = 0.8145**.\n",
    "- **Action:** The code below sets up the search space, runs the tuning, and retrains the model with these best parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets random seeds for reproducibility and defines paths for the CatBoost training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = r'dataProcessing/Final.csv'\n",
    "CODEBOOK_PATH = r'codebook/FinalCodeBook.csv'\n",
    "MODEL_DIR = r'models'\n",
    "\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"PHASE 1: DATA PREPARATION\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the final merged dataset for analysis/training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "print(f\"Data loaded: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parses the codebook to identify and separate Categorical and Numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will parse the Codebook to identify 'Nominal' as Categorical and 'Scale' as Numerical\n",
    "print(\"Parsing Codebook for feature types...\")\n",
    "codebook = pd.read_csv(CODEBOOK_PATH, encoding='latin-1')\n",
    "\n",
    "# Create dictionaries for feature types\n",
    "cat_features = []\n",
    "num_features = []\n",
    "\n",
    "# Target variable\n",
    "target = 'Fail'\n",
    "\n",
    "# Features to exclude (IDs, weights, target, auxiliary)\n",
    "exclude_cols = ['IDCNTRY', 'IDBOOK', 'IDSCHOOL', 'IDCLASS', 'IDSTUD', \n",
    "                'IDTEALIN', 'IDTEACH', 'IDLINK', 'IDPOP', 'IDGRADER', \n",
    "                'IDGRADE', 'IDSUBJ', 'MATWGT', 'JKREP', 'JKZONE', \n",
    "                'Math_Mean', 'Fail']\n",
    "\n",
    "# Scan dataset columns and match with codebook\n",
    "for col in df.columns:\n",
    "    if col in exclude_cols:\n",
    "        continue\n",
    "    \n",
    "    # Find variable in codebook\n",
    "    # We strip whitespace just in case\n",
    "    var_info = codebook[codebook.iloc[:, 0].astype(str).str.strip() == col]\n",
    "    \n",
    "    if not var_info.empty:\n",
    "        # Use numerical index for Level (column 3 based on inspection)\n",
    "        # Variable,Label,Question Location,Level,...\n",
    "        level = var_info.iloc[0, 3]\n",
    "        if level == 'Nominal':\n",
    "            cat_features.append(col)\n",
    "        elif level == 'Scale':\n",
    "            num_features.append(col)\n",
    "        else:\n",
    "            # Fallback: check dtype\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                num_features.append(col)\n",
    "            else:\n",
    "                cat_features.append(col)\n",
    "    else:\n",
    "        # If not in codebook, guess based on dtype\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            num_features.append(col)\n",
    "        else:\n",
    "            cat_features.append(col)\n",
    "\n",
    "print(f\"Categorical features ({len(cat_features)}): {cat_features}\")\n",
    "print(f\"Numerical features ({len(num_features)}): {num_features}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handles missing values: fills categorical with 'Missing' and numerical with median (for CatBoost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost requires categorical features to be strings or integers.\n",
    "# We fill NaNs with a placeholder string to treat missingness as a category\n",
    "print(\"Handling missing values for categorical features...\")\n",
    "for col in cat_features:\n",
    "    df[col] = df[col].astype(str).replace('nan', 'Missing')\n",
    "\n",
    "# Fill numerical missing values with median (simple imputation)\n",
    "print(\"Handling missing values for numerical features...\")\n",
    "for col in num_features:\n",
    "    # Ensure numeric type first, coercing errors to NaN\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df[col] = df[col].fillna(df[col].median())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits the data into Training (70%), Validation (15%), and Test (15%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Splitting data...\")\n",
    "X = df[cat_features + num_features]\n",
    "y = df[target]\n",
    "\n",
    "# First split: Train (70%) vs Temp (30%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: Validation (15% of total -> 50% of Temp) vs Test (15% of total -> 50% of Temp)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=RANDOM_SEED, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training Set: {X_train.shape}\")\n",
    "print(f\"Validation Set: {X_val.shape}\")\n",
    "print(f\"Test Set: {X_test.shape}\")\n",
    "\n",
    "# Create CatBoost Pools\n",
    "train_pool = Pool(X_train, y_train, cat_features=cat_features)\n",
    "val_pool = Pool(X_val, y_val, cat_features=cat_features)\n",
    "test_pool = Pool(X_test, y_test, cat_features=cat_features)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PHASE 2: TRAINING (BASELINE)\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Initializing Baseline CatBoostClassifier (GPU)...\")\n",
    "baseline_model = CatBoostClassifier(\n",
    "    task_type=\"GPU\",\n",
    "    iterations=1000,\n",
    "    learning_rate=0.03,\n",
    "    depth=6,\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='AUC',\n",
    "    random_seed=RANDOM_SEED,\n",
    "    verbose=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Train with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Training Baseline Model...\")\n",
    "start_time = time.time()\n",
    "baseline_model.fit(\n",
    "    train_pool,\n",
    "    eval_set=val_pool,\n",
    "    early_stopping_rounds=50,\n",
    "    use_best_model=True\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"Baseline Training Time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baseline_path = os.path.join(MODEL_DIR, 'catboost_baseline.cbm')\n",
    "baseline_model.save_model(baseline_path)\n",
    "print(f\"Baseline model saved to {baseline_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PHASE 3: HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up RandomizedSearchCV...\")\n",
    "\n",
    "# Hyperparameter tuning\n",
    "# Reduced search space for faster demonstration\n",
    "param_dist = {\n",
    "    'iterations': [500],\n",
    "    'depth': [4, 6],\n",
    "    'learning_rate': [0.03, 0.1],\n",
    "    'l2_leaf_reg': [1, 3]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a base model for tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Note: For GridSearchCV/RandomizedSearchCV with CatBoost, it's often better to pass CPU task_type for the search controller \n",
    "# but the estimator can use GPU. However, sklearn cross-validation splits data which forces CatBoost to re-upload to GPU many times.\n",
    "# We will use a simplified approach: use the built-in randomized_search or grid_search from CatBoost library if possible, \n",
    "# or use sklearn's RandomizedSearchCV with GPU enabled on the estimator.\n",
    "\n",
    "# Using CatBoost's built-in randomized_search\n",
    "print(\"Starting Hyperparameter Tuning using CatBoost's built-in randomized_search...\")\n",
    "start_time = time.time()\n",
    "\n",
    "tuning_model = CatBoostClassifier(\n",
    "    task_type=\"GPU\",\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='AUC',\n",
    "    cat_features=cat_features,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    verbose=0,\n",
    "    early_stopping_rounds=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits the data into Training (70%), Validation (15%), and Test (15%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost expects params in a different format for randomized_search\n",
    "# It returns a dictionary with 'params' key containing the best parameters\n",
    "tuned_result = tuning_model.randomized_search(\n",
    "    param_distributions=param_dist,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    cv=2,\n",
    "    n_iter=2,\n",
    "    partition_random_seed=RANDOM_SEED,\n",
    "    calc_cv_statistics=True,\n",
    "    search_by_train_test_split=False, # Use CV\n",
    "    verbose=False,\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Hyperparameter Tuning Time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "best_params = tuned_result['params']\n",
    "print(f\"Best Parameters found: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Retraining Tuned Model with Best Parameters...\")\n",
    "tuned_model = CatBoostClassifier(\n",
    "    task_type=\"GPU\",\n",
    "    iterations=best_params.get('iterations', 1000), # Default if not in result\n",
    "    depth=best_params.get('depth', 6),\n",
    "    learning_rate=best_params.get('learning_rate', 0.03),\n",
    "    l2_leaf_reg=best_params.get('l2_leaf_reg', 3),\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='AUC',\n",
    "    random_seed=RANDOM_SEED,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "tuned_model.fit(\n",
    "    train_pool,\n",
    "    eval_set=val_pool,\n",
    "    early_stopping_rounds=50,\n",
    "    use_best_model=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Summary: Model Evaluation\n",
    "According to the **Model Performance Comparison** table (Section 6.1), this module evaluates the final models.\n",
    "- **Goal:** Compare Baseline vs. Tuned TabNet on the Test set.\n",
    "- **Key Insight:** The Tuned Model improved **Recall by +3.55%** (from 0.2412 to 0.2767), which is critical for identifying at-risk students.\n",
    "- **Action:** The code below saves the tuned model, defines the evaluation metrics (Accuracy, Precision, Recall, F1), and outputs the performance comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates and saves the Confusion Matrix to evaluate model performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_path = os.path.join(MODEL_DIR, 'catboost_tuned.cbm')\n",
    "tuned_model.save_model(tuned_path)\n",
    "print(f\"Tuned model saved to {tuned_path}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PHASE 4: VALIDATION & EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def evaluate_model(model, pool, name=\"Model\"):\n",
    "    y_pred = model.predict(pool)\n",
    "    y_prob = model.predict_proba(pool)[:, 1]\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print(f\"--- {name} Evaluation ---\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    return f1\n",
    "\n",
    "print(\"Evaluating Baseline Model on Test Set...\")\n",
    "f1_baseline = evaluate_model(baseline_model, test_pool, \"Baseline\")\n",
    "\n",
    "print(\"\\nEvaluating Tuned Model on Test Set...\")\n",
    "f1_tuned = evaluate_model(tuned_model, test_pool, \"Tuned\")\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(f\"Baseline F1: {f1_baseline:.4f}\")\n",
    "print(f\"Tuned F1:    {f1_tuned:.4f}\")\n",
    "if f1_tuned > f1_baseline:\n",
    "    print(\"Result: Tuned model outperformed Baseline.\")\n",
    "else:\n",
    "    print(\"Result: Tuned model did not outperform Baseline (might need more search iterations).\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PHASE 5: MODEL INTERPRETATION\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_importances = tuned_model.get_feature_importance(train_pool)\n",
    "feature_names = X_train.columns\n",
    "fi_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "fi_df = fi_df.sort_values(by='Importance', ascending=False).head(10)\n",
    "\n",
    "print(\"Top 10 Important Features:\")\n",
    "print(fi_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_feature = fi_df.iloc[0]['Feature']\n",
    "print(f\"\\nInterpretation Analysis:\")\n",
    "print(f\"The most influential feature for predicting student failure is '{top_feature}'.\")\n",
    "print(\"This suggests that this specific background factor plays a critical role in academic performance.\")\n",
    "print(\"Educators should focus on monitoring these high-impact variables to intervene early.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXECUTION COMPLETE\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed for reproducibility (Must match train_catboost.py)·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "# Paths\n",
    "DATA_PATH = r'dataProcessing/Final.csv'\n",
    "CODEBOOK_PATH = r'codebook/FinalCodeBook.csv'\n",
    "MODEL_DIR = r'models'\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, 'catboost_tuned.cbm')\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"GENERATING VISUALIZATION ARTIFACTS\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the final merged dataset for analysis/training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parses the codebook to identify and separate Categorical and Numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Parsing Codebook for feature types...\")\n",
    "codebook = pd.read_csv(CODEBOOK_PATH, encoding='latin-1')\n",
    "\n",
    "cat_features = []\n",
    "num_features = []\n",
    "target = 'Fail'\n",
    "exclude_cols = ['IDCNTRY', 'IDBOOK', 'IDSCHOOL', 'IDCLASS', 'IDSTUD', \n",
    "                'IDTEALIN', 'IDTEACH', 'IDLINK', 'IDPOP', 'IDGRADER', \n",
    "                'IDGRADE', 'IDSUBJ', 'MATWGT', 'JKREP', 'JKZONE', \n",
    "                'Math_Mean', 'Fail']\n",
    "\n",
    "for col in df.columns:\n",
    "    if col in exclude_cols:\n",
    "        continue\n",
    "    \n",
    "    var_info = codebook[codebook.iloc[:, 0].astype(str).str.strip() == col]\n",
    "    \n",
    "    if not var_info.empty:\n",
    "        level = var_info.iloc[0, 3]\n",
    "        if level == 'Nominal':\n",
    "            cat_features.append(col)\n",
    "        elif level == 'Scale':\n",
    "            num_features.append(col)\n",
    "        else:\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                num_features.append(col)\n",
    "            else:\n",
    "                cat_features.append(col)\n",
    "    else:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            num_features.append(col)\n",
    "        else:\n",
    "            cat_features.append(col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handles missing values: fills categorical with 'Missing' and numerical with median (for CatBoost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_features:\n",
    "    df[col] = df[col].astype(str).replace('nan', 'Missing')\n",
    "\n",
    "# Handle Numerical Missing Values\n",
    "for col in num_features:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df[col] = df[col].fillna(df[col].median())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits the data into Training (70%), Validation (15%), and Test (15%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[cat_features + num_features]\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=RANDOM_SEED, stratify=y_temp\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the pre-trained CatBoost model from the saved file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "model = CatBoostClassifier()\n",
    "model.load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates feature importance from the model and saves the top features plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating Feature Importance Table...\")\n",
    "feature_importances = model.get_feature_importance(Pool(X_train, y_train, cat_features=cat_features))\n",
    "feature_names = X_train.columns\n",
    "fi_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "fi_df = fi_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Save Feature Importance CSV\n",
    "fi_csv_path = os.path.join(MODEL_DIR, 'feature_importance.csv')\n",
    "fi_df.to_csv(fi_csv_path, index=False)\n",
    "print(f\"Feature Importance CSV saved to: {fi_csv_path}\")\n",
    "\n",
    "# Plot Feature Importance (Top 20)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=fi_df.head(20))\n",
    "plt.title('Top 20 Important Features (CatBoost)')\n",
    "plt.tight_layout()\n",
    "fi_png_path = os.path.join(MODEL_DIR, 'feature_importance.png')\n",
    "plt.savefig(fi_png_path)\n",
    "print(f\"Feature Importance Plot saved to: {fi_png_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates and saves the Confusion Matrix to evaluate model performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating Confusion Matrix...\")\n",
    "y_pred = model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Pass', 'Fail'], \n",
    "            yticklabels=['Pass', 'Fail'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix (Test Set)')\n",
    "cm_png_path = os.path.join(MODEL_DIR, 'confusion_matrix.png')\n",
    "plt.savefig(cm_png_path)\n",
    "print(f\"Confusion Matrix Plot saved to: {cm_png_path}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Artifact generation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 TabNet Advanced Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines feature fusion logic (interactions/squares) and sets global random seeds for TabNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "ENABLE_FEATURE_FUSION = True\n",
    "\n",
    "def add_feature_fusion(df, numeric_cols, max_base_features=20):\n",
    "    if not numeric_cols:\n",
    "        return df, numeric_cols\n",
    "    base_cols = numeric_cols[:max_base_features]\n",
    "    new_numeric_cols = list(numeric_cols)\n",
    "    for col in base_cols:\n",
    "        col_numeric = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        new_col = f\"{col}__sq\"\n",
    "        df[new_col] = col_numeric ** 2\n",
    "        new_numeric_cols.append(new_col)\n",
    "    for c1, c2 in itertools.combinations(base_cols, 2):\n",
    "        c1_numeric = pd.to_numeric(df[c1], errors=\"coerce\")\n",
    "        c2_numeric = pd.to_numeric(df[c2], errors=\"coerce\")\n",
    "        new_col = f\"{c1}__x__{c2}\"\n",
    "        df[new_col] = c1_numeric * c2_numeric\n",
    "        new_numeric_cols.append(new_col)\n",
    "    return df, new_numeric_cols\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "DATA_PATH = r'dataProcessing/Final.csv'\n",
    "CODEBOOK_PATH = r'codebook/FinalCodeBook.csv'\n",
    "OUTPUT_DIR = r'models_tabnet'\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits the data into Training (70%), Validation (15%), and Test (15%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    print(\"\\n[Phase 1] Data Preparation...\")\n",
    "    \n",
    "    # Load Data\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f\"Loaded data shape: {df.shape}\")\n",
    "    \n",
    "    # Identify Target\n",
    "    # User said \"at_risk\" (1=Risk). \n",
    "    # In our data, 'Fail' is 1 (Fail) / 0 (Pass).\n",
    "    if 'Fail' in df.columns:\n",
    "        df.rename(columns={'Fail': 'at_risk'}, inplace=True)\n",
    "    elif 'at_risk' not in df.columns:\n",
    "        # Fallback: assume last column is target if not named Fail/at_risk\n",
    "        print(\"Warning: 'Fail' or 'at_risk' column not found. Using last column as target.\")\n",
    "        df.rename(columns={df.columns[-1]: 'at_risk'}, inplace=True)\n",
    "    \n",
    "    target_col = 'at_risk'\n",
    "    \n",
    "    # Force target to numeric and drop NaNs\n",
    "    df[target_col] = pd.to_numeric(df[target_col], errors='coerce')\n",
    "    df.dropna(subset=[target_col], inplace=True)\n",
    "    df[target_col] = df[target_col].astype(int)\n",
    "    \n",
    "    print(f\"Target distribution:\\n{df[target_col].value_counts(normalize=True)}\")\n",
    "\n",
    "    # Load Codebook to determine allowed features and types\n",
    "    try:\n",
    "        cb = pd.read_csv(CODEBOOK_PATH, encoding='latin-1')\n",
    "        cb.columns = cb.columns.str.strip().str.replace('^ï»¿', '', regex=True)\n",
    "        var_col = cb.columns[0] # Usually 'Variable'\n",
    "        \n",
    "        allowed_vars = set(cb[var_col].dropna().astype(str).str.strip().tolist())\n",
    "        print(f\"Allowed variables from codebook: {len(allowed_vars)}\")\n",
    "        \n",
    "        # Filter DataFrame to keep only allowed vars + Target\n",
    "        # Also ensure we don't drop the target if it's not in allowed_vars (though it should be)\n",
    "        keep_cols = [c for c in df.columns if c in allowed_vars or c == target_col]\n",
    "        df = df[keep_cols]\n",
    "        print(f\"Filtered data shape (codebook vars only): {df.shape}\")\n",
    "        \n",
    "        type_map = dict(zip(cb[var_col], cb['Level']))\n",
    "    except Exception as e:\n",
    "        print(f\"Codebook reading failed ({e}), using all columns.\")\n",
    "        allowed_vars = set(df.columns)\n",
    "        type_map = {}\n",
    "\n",
    "    # Drop ID columns and Weights if they exist (heuristics based on previous file knowledge)\n",
    "    # Even after filtering, double check to remove IDs if they somehow got into codebook\n",
    "    drop_cols = [c for c in df.columns if c.startswith('ID') or 'WGT' in c or c == target_col]\n",
    "    feature_cols = [c for c in df.columns if c not in drop_cols and c != target_col]\n",
    "    \n",
    "    numeric_cols = []\n",
    "    categorical_cols = []\n",
    "        \n",
    "    for col in feature_cols:\n",
    "        level = type_map.get(col, 'Unknown')\n",
    "        if level == 'Scale':\n",
    "            numeric_cols.append(col)\n",
    "        elif level == 'Nominal' or level == 'Ordinal':\n",
    "            categorical_cols.append(col)\n",
    "        else:\n",
    "            # Fallback heuristic\n",
    "            if pd.api.types.is_numeric_dtype(df[col]) and df[col].nunique() > 10:\n",
    "                numeric_cols.append(col)\n",
    "            else:\n",
    "                categorical_cols.append(col)\n",
    "\n",
    "    if ENABLE_FEATURE_FUSION:\n",
    "        df, numeric_cols = add_feature_fusion(df, numeric_cols)\n",
    "        feature_cols = [c for c in df.columns if c not in drop_cols and c != target_col]\n",
    "\n",
    "    print(f\"Numeric features ({len(numeric_cols)}): {numeric_cols}\")\n",
    "    print(f\"Categorical features ({len(categorical_cols)}): {categorical_cols}\")\n",
    "    \n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target_col].copy()\n",
    "\n",
    "    # Split first (70% Train, 30% Temp)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.3, stratify=y, random_state=SEED\n",
    "    )\n",
    "    # Split Temp (50% Valid, 50% Test -> 15% Valid, 15% Test of total)\n",
    "    X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=SEED\n",
    "    )\n",
    "    \n",
    "    # Missing Value Imputation\n",
    "    # Numeric -> Median (fit on Train)\n",
    "    for col in numeric_cols:\n",
    "        # Force to numeric (coerce errors to NaN)\n",
    "        X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n",
    "        X_valid[col] = pd.to_numeric(X_valid[col], errors='coerce')\n",
    "        X_test[col] = pd.to_numeric(X_test[col], errors='coerce')\n",
    "        \n",
    "        median_val = X_train[col].median()\n",
    "        X_train[col].fillna(median_val, inplace=True)\n",
    "        X_valid[col].fillna(median_val, inplace=True)\n",
    "        X_test[col].fillna(median_val, inplace=True)\n",
    "        \n",
    "    # Categorical -> Mode or \"Missing\" (fit on Train)\n",
    "    for col in categorical_cols:\n",
    "        # If numeric-like categorical (int codes), fill with -1 or mode.\n",
    "        # If string, fill \"Missing\".\n",
    "        if pd.api.types.is_numeric_dtype(X_train[col]):\n",
    "            fill_val = -1 # Common for int-encoded categories\n",
    "        else:\n",
    "            fill_val = \"Missing\"\n",
    "        \n",
    "        X_train[col].fillna(fill_val, inplace=True)\n",
    "        X_valid[col].fillna(fill_val, inplace=True)\n",
    "        X_test[col].fillna(fill_val, inplace=True)\n",
    "\n",
    "    # Encoding\n",
    "    # Numeric -> StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    if numeric_cols:\n",
    "        X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "        X_valid[numeric_cols] = scaler.transform(X_valid[numeric_cols])\n",
    "        X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "    \n",
    "    # Categorical -> LabelEncoder\n",
    "    # TabNet needs positive integers 0..N-1\n",
    "    cat_idxs = []\n",
    "    cat_dims = []\n",
    "    \n",
    "    # Map feature names to indices for TabNet\n",
    "    feature_names = list(X_train.columns)\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        # Fit on all possible values to avoid unknown class error, OR handle unknowns\n",
    "        # Best practice: Fit on Train, handle unknown in Valid/Test\n",
    "        le.fit(X_train[col].astype(str))\n",
    "        \n",
    "        # Helper to safely transform\n",
    "        def safe_transform(series, encoder):\n",
    "            classes = set(encoder.classes_)\n",
    "            # Replace unknown with the first class (or a specific 'unknown' if we had one)\n",
    "            # Here we map unknown to the most frequent (mode) which is usually class 0 after some sorting, \n",
    "            # or just use 0. Better: map to a special 'Unknown' if not present.\n",
    "            # Simplified: Map unknown to class 0\n",
    "            return series.astype(str).apply(lambda x: encoder.transform([x])[0] if x in classes else 0)\n",
    "\n",
    "        X_train[col] = le.transform(X_train[col].astype(str))\n",
    "        X_valid[col] = safe_transform(X_valid[col], le)\n",
    "        X_test[col] = safe_transform(X_test[col], le)\n",
    "        \n",
    "        cat_idxs.append(feature_names.index(col))\n",
    "        cat_dims.append(len(le.classes_))\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train.values, 'y_train': y_train.values,\n",
    "        'X_valid': X_valid.values, 'y_valid': y_valid.values,\n",
    "        'X_test': X_test.values, 'y_test': y_test.values,\n",
    "        'cat_idxs': cat_idxs, 'cat_dims': cat_dims,\n",
    "        'feature_names': feature_names\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training (Baseline):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines the function to train a baseline TabNet model with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(data):\n",
    "    print(\"\\n[Phase 2] Training Baseline TabNet...\")\n",
    "    \n",
    "    clf = TabNetClassifier(\n",
    "        seed=SEED,\n",
    "        cat_idxs=data['cat_idxs'],\n",
    "        cat_dims=data['cat_dims'],\n",
    "        cat_emb_dim=1, # Default is 1, can be tuned\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=2e-2),\n",
    "        scheduler_params={\"step_size\":10, \"gamma\":0.9},\n",
    "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "        mask_type='sparsemax', # This is useful for interpretability\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    clf.fit(\n",
    "        X_train=data['X_train'], y_train=data['y_train'],\n",
    "        eval_set=[(data['X_train'], data['y_train']), (data['X_valid'], data['y_valid'])],\n",
    "        eval_name=['train', 'valid'],\n",
    "        eval_metric=['auc', 'accuracy'],\n",
    "        max_epochs=50, # Reduced for demo speed, typically 100+\n",
    "        patience=10,\n",
    "        batch_size=1024, \n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "    print(f\"Baseline Training Time: {time.time() - start_time:.2f}s\")\n",
    "    print(f\"Best Valid Score: {clf.best_cost}\")\n",
    "    \n",
    "    save_path = os.path.join(OUTPUT_DIR, 'tabnet_baseline.zip')\n",
    "    clf.save_model(save_path)\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines the hyperparameter tuning function using Randomized Search to find optimal TabNet parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(data):\n",
    "    print(\"\\n[Phase 3] Tuning (Randomized Search Manual Loop)...\")\n",
    "    \n",
    "    # Parameter Space\n",
    "    param_grid = {\n",
    "        'n_d': [8, 16, 24],\n",
    "        'n_steps': [3, 5],\n",
    "        'gamma': [1.0, 1.5],\n",
    "        'lambda_sparse': [0, 1e-4],\n",
    "        'learning_rate': [0.01, 0.02],\n",
    "    }\n",
    "    \n",
    "    # Generate random combinations (e.g., 5 trials)\n",
    "    import itertools\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    all_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    # Randomly sample 5\n",
    "    search_space = random.sample(all_combinations, min(5, len(all_combinations)))\n",
    "    \n",
    "    best_score = -1\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for i, params in enumerate(search_space):\n",
    "        print(f\"Trial {i+1}/{len(search_space)}: {params}\")\n",
    "        \n",
    "        # Set n_a = n_d as per TabNet recommendation\n",
    "        params['n_a'] = params['n_d']\n",
    "        \n",
    "        clf = TabNetClassifier(\n",
    "            seed=SEED,\n",
    "            n_d=params['n_d'],\n",
    "            n_a=params['n_a'],\n",
    "            n_steps=params['n_steps'],\n",
    "            gamma=params['gamma'],\n",
    "            lambda_sparse=params['lambda_sparse'],\n",
    "            cat_idxs=data['cat_idxs'],\n",
    "            cat_dims=data['cat_dims'],\n",
    "            optimizer_params=dict(lr=params['learning_rate']),\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        clf.fit(\n",
    "            X_train=data['X_train'], y_train=data['y_train'],\n",
    "            eval_set=[(data['X_valid'], data['y_valid'])],\n",
    "            eval_metric=['auc'],\n",
    "            max_epochs=30,\n",
    "            patience=5,\n",
    "            batch_size=1024,\n",
    "            virtual_batch_size=128\n",
    "        )\n",
    "        \n",
    "        val_auc = clf.best_cost\n",
    "        results.append({'params': params, 'val_auc': val_auc})\n",
    "        print(f\"  -> Valid AUC: {val_auc}\")\n",
    "        \n",
    "        if val_auc > best_score:\n",
    "            best_score = val_auc\n",
    "            best_params = params\n",
    "            best_model = clf\n",
    "            \n",
    "    print(f\"\\nBest Params: {best_params}\")\n",
    "    print(f\"Best Valid AUC: {best_score}\")\n",
    "    \n",
    "    # Save Best Model\n",
    "    save_path = os.path.join(OUTPUT_DIR, 'tabnet_tuned.zip')\n",
    "    best_model.save_model(save_path)\n",
    "    \n",
    "    # Save Tuning Logs\n",
    "    pd.DataFrame(results).to_csv(os.path.join(OUTPUT_DIR, 'tuning_results.csv'), index=False)\n",
    "    \n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines the evaluation function to calculate and save metrics (Accuracy, Precision, Recall, F1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data, model_name=\"Model\"):\n",
    "    print(f\"\\n[Phase 4] Evaluating {model_name}...\")\n",
    "    \n",
    "    # Predict\n",
    "    preds = model.predict(data['X_test'])\n",
    "    # TabNet predict_proba returns [prob_0, prob_1]\n",
    "    probs = model.predict_proba(data['X_test'])[:, 1]\n",
    "    \n",
    "    y_true = data['y_test']\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_true, preds)\n",
    "    prec = precision_score(y_true, preds)\n",
    "    rec = recall_score(y_true, preds)\n",
    "    f1 = f1_score(y_true, preds)\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f} (Crucial for At-Risk)\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, preds))\n",
    "    \n",
    "    # Save Metrics\n",
    "    metrics = {\n",
    "        'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1': f1\n",
    "    }\n",
    "    with open(os.path.join(OUTPUT_DIR, f'{model_name}_metrics.json'), 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "        \n",
    "    # Confusion Matrix Plot\n",
    "    cm = confusion_matrix(y_true, preds)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, ['Not Risk', 'At Risk'])\n",
    "    plt.yticks(tick_marks, ['Not Risk', 'At Risk'])\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, f'{model_name}_cm.png'))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines the interpretation function to extract and save feature importance from the TabNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_model(model, data):\n",
    "    print(\"\\n[Phase 5] Feature Importance...\")\n",
    "    \n",
    "    feat_importances = model.feature_importances_\n",
    "    indices = np.argsort(feat_importances)[::-1]\n",
    "    \n",
    "    print(\"All Feature Importances:\")\n",
    "    top_feats = []\n",
    "    for i in range(len(indices)):\n",
    "        feat_name = data['feature_names'][indices[i]]\n",
    "        score = feat_importances[indices[i]]\n",
    "        print(f\"{i+1}. {feat_name}: {score:.4f}\")\n",
    "        top_feats.append({'Feature': feat_name, 'Importance': score})\n",
    "        \n",
    "    pd.DataFrame(top_feats).to_csv(os.path.join(OUTPUT_DIR, 'feature_importance.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main execution block: Runs the full TabNet pipeline (Prepare, Train, Tune, Evaluate, Interpret)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1. Prepare\n",
    "    data = load_and_preprocess_data()\n",
    "    \n",
    "    # 2. Train Baseline\n",
    "    baseline_model = train_baseline(data)\n",
    "    \n",
    "    # 3. Tune\n",
    "    tuned_model = tune_model(data)\n",
    "    \n",
    "    # 4. Evaluate\n",
    "    import itertools # Ensure imported for plotting\n",
    "    evaluate_model(baseline_model, data, \"Baseline\")\n",
    "    evaluate_model(tuned_model, data, \"Tuned\")\n",
    "    \n",
    "    # 5. Interpret\n",
    "    interpret_model(tuned_model, data)\n",
    "    \n",
    "    print(f\"\\nAll artifacts saved to {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Project Report & Results\n",
    "\n",
    "### 6.1 Model Performance Comparison\n",
    "| Metric | Baseline Model | Tuned Model | Improvement |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Accuracy** | 0.8763 | **0.8777** | +0.14% |\n",
    "| **Precision** | **0.6914** | 0.6740 | -1.74% |\n",
    "| **Recall** | 0.2412 | **0.2767** | +3.55% |\n",
    "| **F1 Score** | 0.3576 | **0.3923** | +3.47% |\n",
    "\n",
    "> **Note**: The Tuned Model shows a significant improvement in Recall (+3.55%), which is the most critical metric for identifying at-risk students.\n",
    "\n",
    "### 6.2 Hyperparameter Tuning Results (Top 5 Trials)\n",
    "| Trial | n_d | n_steps | gamma | lambda_sparse | lr | Val AUC |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| 1 | 24 | 5 | 1.0 | 0 | 0.01 | 0.7766 |\n",
    "| 2 | 8 | 3 | 1.5 | 0.0001 | 0.02 | 0.7873 |\n",
    "| 3 | 8 | 3 | 1.0 | 0 | 0.02 | 0.7783 |\n",
    "| 4 | 24 | 5 | 1.5 | 0.0001 | 0.02 | 0.6209 |\n",
    "| **5 (Best)** | **16** | **3** | **1.0** | **0** | **0.02** | **0.8145** |\n",
    "\n",
    "### 6.3 Feature Importance (Top Features)\n",
    "| Rank | Feature | Importance | Description |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| 1 | BCBG16B | 0.3856 | Student Absenteeism (Problem Degree) |\n",
    "| 2 | BCBG14G | 0.1571 | Parental Expectations (School Character) |\n",
    "| 3 | BCBG14H | 0.1442 | Parental Support (School Character) |\n",
    "| 4 | BCBG14E | 0.0526 | Parental Involvement (School Character) |\n",
    "| 5 | BCBG15G | 0.0495 | Encourage Students (Agreement) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
